{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from BeyondBlooms2024.config import name_dict, color_dict\n",
    "from BeyondBlooms2024.config_file import (ABUNDANCES_FILE, CCMN_CON_MAP_PATH, CON_LOUVAIN_META_PATH,CON_LOUVAIN_NETWORK_PATH, ENRICH,\n",
    "NUM_PERMUTATIONS, NUM_SAMPLES, NUM_CORES, METADATA_FILE, PRUNED_PVAL_CCMN_PATH,PVAL_CCMN_PATH,ENRICHED_META_PATH, RANDOM_PVAL_CCMN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_egc_03L = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final_proj/EGC_03-L-Fig5_Project_Condtions__final__-12-|-01-|-02-_F4_EGCRaw_sstable.csv\", sep=\",\").T\n",
    "df_ss_egc_03L"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_f4_03L = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final_proj/F4_03-L-Fig5_Project_Condtions__final__-12-|-01-|-02-_F4Raw_sstable.csv\", sep=\",\").T\n",
    "df_ss_f4_03L"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_egc_10H = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final_proj/EGC_Test_10-H-Fig5_Project_Condtions__final__-06-|-07-|-08-_F4_EGCRaw_sstable.csv\", sep=\",\").T\n",
    "df_ss_egc_10H"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_f4_10H = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final_proj/F4_Test_10-H-Fig5_Project_Condtions__final__-06-|-07-|-08-_F4Raw_sstable.csv\", sep=\",\").T\n",
    "df_ss_f4_10H"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_atlantic_3 = df_ss_f4_03L[(df_ss_f4_03L[0]==1) | (df_ss_f4_03L[1]==1) | (df_ss_f4_03L[2]==1) | (df_ss_f4_03L[3]==1)].index.to_list()[1:]\n",
    "list_of_arctic_3 = df_ss_egc_03L[(df_ss_egc_03L[0]==1)].index.to_list()[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_atlantic_10 = df_ss_f4_10H[(df_ss_f4_10H[0]==1) | (df_ss_f4_10H[1]==1) | (df_ss_f4_10H[2]==1)].index.to_list()[1:]\n",
    "list_of_arctic_10 = df_ss_egc_10H[df_ss_egc_10H[0]==1].index.to_list()[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_arctic_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_f4_03L_NON = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final/REla_with_env__mod_withoutSeason_03LW_-12-|-01-|-02-_tr0.01Raw_sstable.csv\", sep=\",\").T\n",
    "df_ss_f4_03L_NON"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_atlantic_3_NON = df_ss_f4_03L_NON[(df_ss_f4_03L_NON[0]==1) | (df_ss_f4_03L_NON[1]==1) | (df_ss_f4_03L_NON[2]==1) | (df_ss_f4_03L_NON[3]==1)].index.to_list()[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ss_f4_10H_NON = pd.read_csv(\"CCMN_Paper_Submission/tables/woSeason_final/REla_with_env__mod_withoutSeason_10HS_-06-|-07-|-08-_tr0.02Raw_sstable.csv\", sep=\",\").T\n",
    "df_ss_f4_10H_NON"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_of_atlantic_10_NON = df_ss_f4_10H_NON[(df_ss_f4_10H_NON[0]==1) | (df_ss_f4_10H_NON[1]==1) | (df_ss_f4_10H_NON[2]==1)].index.to_list()[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Origin Summer\",len(list_of_atlantic_10_NON))\n",
    "print(\"Atlantic Summer\",len(list_of_atlantic_10))\n",
    "print(\"Arctic Summer\",len(list_of_arctic_10))\n",
    "print(\"Origin Winter\",len(list_of_atlantic_3_NON))\n",
    "print(\"Atlantic Winter\",len(list_of_atlantic_3))\n",
    "print(\"Arctic Winter\",len(list_of_arctic_3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "# ccmn edges (nmi)\n",
    "df_search_all = pd.read_csv(PVAL_CCMN_PATH , sep=\";\", index_col=0)\n",
    "df_search_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_search_meta = pd.read_csv(ENRICH, sep=\",\")\n",
    "node_clu_dict_ = df_search_meta[[\"Nodes\", \"cluster_names\"]]\n",
    "node_clu_dict_.set_index(\"Nodes\", inplace=True)\n",
    "node_clu_dict =node_clu_dict_.to_dict()[\"cluster_names\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_search_all[\"from_clu\"]=df_search_all[\"from\"].map(node_clu_dict)\n",
    "df_search_all[\"to_clu\"]=df_search_all[\"to\"].map(node_clu_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_stable(x, l1):\n",
    "    if x in l1:\n",
    "        ret = 1\n",
    "    else:\n",
    "        ret = 0\n",
    "    return ret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clu_ = \"03LW\"\n",
    "df_temp_clu = df_search_all[(df_search_all[\"from_clu\"]== clu_) & (df_search_all[\"to_clu\"]== clu_)]\n",
    "df_temp_clu[\"1-corr\"]= 1-df_temp_clu[\"corr\"]\n",
    "G = nx.from_pandas_edgelist(df_temp_clu, 'from', 'to', ['1-corr'], create_using=nx.DiGraph())\n",
    "# Compute betweenness centrality for each node\n",
    "betweenness = nx.betweenness_centrality(G, weight =\"1-corr\")\n",
    "\n",
    "# Compute closeness centrality for each node\n",
    "closeness = nx.closeness_centrality(G , distance =\"1-corr\")\n",
    "\n",
    "# Display the betweenness centrality and closeness centrality for each node\n",
    "list_of_nodes = []\n",
    "list_of_BC = []\n",
    "list_of_CC = []\n",
    "for node in G.nodes():\n",
    "    #print(f\"Node {node}:\")\n",
    "    #print(f\"Betweenness Centrality: {betweenness[node]}\")\n",
    "    #print(f\"Closeness Centrality: {closeness[node]}\")\n",
    "    #print(\"------------------------\")\n",
    "    list_of_nodes.append(node)\n",
    "    list_of_BC.append(betweenness[node])\n",
    "    list_of_CC.append(closeness[node])\n",
    "df_03 = pd.DataFrame(list_of_nodes)\n",
    "df_03[\"Betweenness_Centrality\"] = list_of_BC\n",
    "df_03[\"Closeness Centrality\"] = list_of_CC\n",
    "df_03.rename(columns={0: \"Asv\"}, inplace=True)\n",
    "df_03[\"atlantic\"] = df_03[\"Asv\"].apply(lambda x: is_stable(x, list_of_atlantic_3))\n",
    "df_03[\"arctic\"]= df_03[\"Asv\"].apply(lambda x: is_stable(x, list_of_arctic_3))\n",
    "df_03[\"origin\"]= df_03[\"Asv\"].apply(lambda x: is_stable(x, list_of_atlantic_3_NON))\n",
    "df_03"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clu_2 = \"10HS\"\n",
    "df_temp_clu2 = df_search_all[(df_search_all[\"from_clu\"]== clu_2) & (df_search_all[\"to_clu\"]== clu_2)]\n",
    "df_temp_clu2[\"1-corr\"]= 1-df_temp_clu2[\"corr\"]\n",
    "G2 = nx.from_pandas_edgelist(df_temp_clu2, 'from', 'to', ['1-corr'], create_using=nx.DiGraph())\n",
    "# Compute betweenness centrality for each node\n",
    "betweenness2 = nx.betweenness_centrality(G2, weight =\"1-corr\")\n",
    "\n",
    "# Compute closeness centrality for each node\n",
    "closeness2 = nx.closeness_centrality(G2 , distance =\"1-corr\")\n",
    "\n",
    "# Display the betweenness centrality and closeness centrality for each node\n",
    "list_of_nodes2 = []\n",
    "list_of_BC2 = []\n",
    "list_of_CC2 = []\n",
    "for node in G2.nodes():\n",
    "    #print(f\"Node {node}:\")\n",
    "    #print(f\"Betweenness Centrality: {betweenness[node]}\")\n",
    "    #print(f\"Closeness Centrality: {closeness[node]}\")\n",
    "    #print(\"------------------------\")\n",
    "    list_of_nodes2.append(node)\n",
    "    list_of_BC2.append(betweenness2[node])\n",
    "    list_of_CC2.append(closeness2[node])\n",
    "df_10 = pd.DataFrame(list_of_nodes2)\n",
    "df_10[\"Betweenness_Centrality\"] = list_of_BC2\n",
    "df_10[\"Closeness Centrality\"] = list_of_CC2\n",
    "df_10.rename(columns={0: \"Asv\"}, inplace=True)\n",
    "df_10[\"atlantic\"] = df_10[\"Asv\"].apply(lambda x: is_stable(x, list_of_atlantic_10))\n",
    "df_10[\"arctic\"]= df_10[\"Asv\"].apply(lambda x: is_stable(x, list_of_arctic_10))\n",
    "df_10[\"origin\"]= df_10[\"Asv\"].apply(lambda x: is_stable(x, list_of_atlantic_10_NON))\n",
    "df_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_10[\"cluster\"] =\"10HS\"\n",
    "df_03[\"cluster\"] =\"03LW\"\n",
    "df_10_3 = pd.concat([df_10, df_03])\n",
    "df_10_3[\"Stable_State_Atlantic\"] = df_10_3[\"atlantic\"]\n",
    "df_10_3[\"Stable_State_Arctic\"] = df_10_3[\"arctic\"]\n",
    "df_10_3[\"Stable_State_Origin\"] = df_10_3[\"origin\"]\n",
    "df_10_3.drop(columns=[\"atlantic\"], inplace=True)\n",
    "df_10_3.drop(columns=[\"arctic\"], inplace=True)\n",
    "df_10_3.drop(columns=[\"origin\"], inplace=True)\n",
    "df_10_3.set_index(\"Asv\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "atlantic_color= '#B82092'\n",
    "arctic_color = '#2E7FD0'\n",
    "origin_color = '#5D5FBE'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_10_3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "SMALL_SIZE = 24\n",
    "MEDIUM_SIZE = 24\n",
    "BIGGER_SIZE = 24\n",
    "MEDIUM= 24\n",
    "TICK = 24\n",
    "I_SIZE = 8\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM)   # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
    "plt.rcParams['axes.spines.left'] = True\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "plt.rcParams['axes.spines.bottom'] = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_non_ccm_asvs(df, li):\n",
    "    nodes = set(df[\"to\"].to_list()+df[\"from\"].to_list())\n",
    "    sli = set(li)\n",
    "    set_d =sli.difference(nodes)\n",
    "    sec = sli.intersection(nodes)\n",
    "    return sec, set_d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_table(list_of_asv=[], df_search=df_search_all, mod=\"Not_ALL\"):\n",
    "    if mod == \"Not_ALL\":\n",
    "        list_of_ccm_asvs, list_non_ccm =get_non_ccm_asvs(df_search, list_of_asv)\n",
    "        print(\"input asvs:\", list_of_asv)\n",
    "        print(\"Found in the CON ASVs:\", len(list_of_ccm_asvs))\n",
    "        print(\"Gone ASVs:\", len(list_non_ccm))\n",
    "        df_mew = pd.DataFrame(list(list_of_ccm_asvs))\n",
    "\n",
    "    df_mew[\"cluster\"] = df_mew.apply(lambda x:df_search[df_search[\"from\"]==x[0]][\"from_clu\"].mode(), axis=1)\n",
    "    df_mew[\"cluster\"] = df_mew[\"cluster\"]\n",
    "    df_mew[\"number_connec_in\"] = df_mew.apply(lambda x:df_search[(df_search[\"to\"]==x[0]) & (df_search[\"from_clu\"]==df_search[\"to_clu\"])][\"from\"].count(), axis=1)\n",
    "    df_mew[\"number_connec_out\"] = df_mew.apply(lambda x:df_search[(df_search[\"from\"]==x[0]) & (df_search[\"from_clu\"]!=df_search[\"to_clu\"])][\"from\"].count(), axis=1) #todo\n",
    "    all_clusters = set(df_search[\"from_clu\"].to_list()).union(set(df_search[\"to_clu\"].to_list()))\n",
    "    for clu in list(all_clusters):\n",
    "        df_mew[f\"from_clu_{clu}\"] = df_mew.apply(lambda x:df_search[(df_search[\"from\"]==x[0]) & (df_search[\"to_clu\"]==clu)][\"corr\"].mean(), axis=1)\n",
    "        df_mew[f\"to_clu_{clu}\"] = df_mew.apply(lambda x:df_search[(df_search[\"to\"]==x[0]) & (df_search[\"from_clu\"]==clu)][\"corr\"].mean(), axis=1)\n",
    "        df_mew[f\"number_clu_connec_{clu}\"]= df_mew.apply(lambda x:df_search[(df_search[\"from\"]==x[0]) & (df_search[\"to_clu\"]==clu)][\"from\"].count(), axis=1)\n",
    "    return df_mew"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_list_of_atlantic_10_NON = create_table(list_of_asv=list_of_atlantic_10_NON)\n",
    "df_list_of_atlantic_10_NON"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Origin Summer\",len(list_of_atlantic_10_NON))\n",
    "print(\"Atlantic Summer\",len(list_of_atlantic_10))\n",
    "print(\"Arctic Summer\",len(list_of_arctic_10))\n",
    "print(\"Origin Winter\",len(list_of_atlantic_3_NON))\n",
    "print(\"Atlantic Winter\",len(list_of_atlantic_3))\n",
    "print(\"Arctic Winter\",len(list_of_arctic_3))\n",
    "\n",
    "df_arctic_3 = create_table(list_of_asv=list_of_arctic_3)\n",
    "sum_df_arctic_3 = df_arctic_3.describe()\n",
    "\n",
    "df_atlantic_3 = create_table(list_of_asv=list_of_atlantic_3)\n",
    "sum_df_atlantic_3 = df_atlantic_3.describe()\n",
    "\n",
    "df_origin_3 = create_table(list_of_asv=list_of_atlantic_3_NON)\n",
    "sum_df_origin_3 = df_origin_3.describe()\n",
    "\n",
    "df_arctic_10 = create_table(list_of_asv=list_of_arctic_10)\n",
    "sum_df_arctic_10 = df_arctic_10.describe()\n",
    "\n",
    "df_atlantic_10 = create_table(list_of_asv=list_of_atlantic_10)\n",
    "sum_df_atlantic_10 = df_atlantic_10.describe()\n",
    "\n",
    "df_origin_10 = create_table(list_of_asv=list_of_atlantic_10_NON)\n",
    "sum_df_origin_10 = df_origin_10.describe()\n",
    "#####\n",
    "list_of_all_asv_3 = df_ss_f4_03L_NON.index.to_list()\n",
    "df_allASV_3 = create_table(list_of_asv=list_of_all_asv_3)\n",
    "sum_df_allASV_3 = df_allASV_3.describe()\n",
    "\n",
    "list_of_all_asv_10 = df_ss_f4_10H_NON.index.to_list()\n",
    "df_allASV_10 = create_table(list_of_asv=list_of_all_asv_10)\n",
    "sum_df_allASV_10 = df_allASV_10.describe()\n",
    "\n",
    "custom_palette = {\"arctic\": arctic_color, \"atlantic\": atlantic_color, \"origin\": origin_color, \"all ASVs\": \"darkgrey\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a figure and subplots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "df_arctic_3[\"env\"]=\"arctic\"\n",
    "df_arctic_3[\"to_nmi\"]=df_arctic_3['to_clu_03LW']\n",
    "df_arctic_3[\"from_nmi\"]=df_arctic_3['from_clu_03LW']\n",
    "\n",
    "df_atlantic_3[\"env\"]=\"atlantic\"\n",
    "df_atlantic_3[\"from_nmi\"]=df_atlantic_3['from_clu_03LW']\n",
    "df_atlantic_3[\"to_nmi\"]=df_atlantic_3['to_clu_03LW']\n",
    "\n",
    "df_origin_3[\"env\"]=\"origin\"\n",
    "df_origin_3[\"from_nmi\"]=df_origin_3['from_clu_03LW']\n",
    "df_origin_3[\"to_nmi\"]=df_origin_3['to_clu_03LW']\n",
    "\n",
    "df_allASV_3[\"env\"]=\"all ASVs\"\n",
    "df_allASV_3[\"from_nmi\"]=df_allASV_3['from_clu_03LW']\n",
    "df_allASV_3[\"to_nmi\"]=df_allASV_3['to_clu_03LW']\n",
    "\n",
    "df_arctic_10[\"env\"]=\"arctic\"\n",
    "df_arctic_10[\"to_nmi\"]=df_arctic_10['to_clu_10HS']\n",
    "df_arctic_10[\"from_nmi\"]=df_arctic_10['from_clu_10HS']\n",
    "\n",
    "df_atlantic_10[\"env\"]=\"atlantic\"\n",
    "df_atlantic_10[\"from_nmi\"]=df_atlantic_10['from_clu_10HS']\n",
    "df_atlantic_10[\"to_nmi\"]=df_atlantic_10['to_clu_10HS']\n",
    "\n",
    "df_origin_10[\"env\"]=\"origin\"\n",
    "df_origin_10[\"from_nmi\"]=df_origin_10['from_clu_10HS']\n",
    "df_origin_10[\"to_nmi\"]=df_origin_10['to_clu_10HS']\n",
    "\n",
    "df_allASV_10[\"env\"]=\"all ASVs\"\n",
    "df_allASV_10[\"from_nmi\"]=df_allASV_10['from_clu_10HS']\n",
    "df_allASV_10[\"to_nmi\"]=df_allASV_10['to_clu_10HS']\n",
    "\n",
    "df_l5 = pd.concat([df_arctic_3, df_atlantic_3, df_arctic_10, df_atlantic_10,df_origin_3,df_origin_10,df_allASV_3, df_allASV_10])\n",
    "print(df_l5.head())\n",
    "# Define the custom sorting order\n",
    "custom_order = [\"all ASVs\", \"origin\", \"atlantic\", \"arctic\"]\n",
    "# Convert the 'color' column to a categorical data type with custom ordering\n",
    "df_l5['env'] = pd.Categorical(df_l5['env'], categories=custom_order, ordered=True)\n",
    "df_l5.rename(columns={'to_nmi':'edge weight nmi (out)'}, inplace=True)\n",
    "# Sort the DataFrame based on the 'color' column\n",
    "df_sorted = df_l5.sort_values(by='env')\n",
    "\n",
    "closedict = df_10_3[\"Closeness Centrality\"].to_dict()\n",
    "betdict = df_10_3[\"Betweenness_Centrality\"].to_dict()\n",
    "df_l5[\"Closeness Centrality\"] =df_l5[0].map(closedict)\n",
    "df_l5[\"Betweenness_Centrality\"] =df_l5[0].map(betdict)\n",
    "# List of columns for plotting\n",
    "columns = [\"Closeness Centrality\",\"Betweenness_Centrality\",'number_connec_in', 'number_connec_out', 'from_nmi', 'to_nmi']\n",
    "columns = [\"Closeness Centrality\", 'edge weight nmi (out)']\n",
    "quartile_colors = ['lightblue', 'royalblue', 'lightblue']\n",
    "# Create violin plots for each column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_l5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
    "df_l5_reset = df_l5.reset_index(drop=True)\n",
    "for i, column in enumerate(columns):\n",
    "    ax = sns.violinplot(data=df_l5_reset, x= \"cluster\",  y=column, ax=axes[i], hue=\"env\", palette=custom_palette )\n",
    "    #ax.gca().legend.remove()\n",
    "    #axes[i].set_title(column)\n",
    "    #axes[i].set_xlabel(\"\")  # Remove x-label for better visualization\n",
    "        # Get the handles and labels for the first subplot to create the legend\n",
    "    if i == 0:\n",
    "        handles, _ = axes[i].get_legend_handles_labels()\n",
    "\n",
    "# Create a custom legend for quartiles\n",
    "quartile_legend = [mpatches.Patch(color='lightblue', label='25th percentile'),\n",
    "                   mpatches.Patch(color='royalblue', label='50th percentile (median)'),\n",
    "                   mpatches.Patch(color='lightblue', label='75th percentile')]\n",
    "# Remove the legend after plotting\n",
    "for ax in axes:\n",
    "    handles, labels = ax.get_legend_handles_labels()  # Get legend handles and labels\n",
    "    ax.legend_.remove()  # Remove the legend\n",
    "# Display the legend\n",
    "#fig.legend(handles=quartile_legend, loc='upper center', bbox_to_anchor=(0.5, 1.08), ncol=3)\n",
    "fig.legend(handles, [\"All ASVs\", \"Origin\", \"Atlantic\", \"Arctic\"], loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "#plt.suptitle('Violin Plots for Different Columns', y=1.05, fontsize=16)\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/Sup_Figure_S15_Arctic_Atlantic_Origin_violin_plot_seperate_ALL.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_l5[(df_l5[\"cluster\"]==\"03LW\") &(df_l5[\"env\"]==\"arctic\")]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "#results: Clearly, ASVs stable in their natural conditions (and in Atlantic conditions) display a significantly () higher centrality than average, while those predicted to be stable under Arctic conditions show a reduced centrality. \n",
    "\n",
    "\n",
    "vector_A =df_l5[(df_l5[\"cluster\"]==\"10HS\") &(df_l5[\"env\"]==\"arctic\")]['edge weight nmi (out)'].dropna().values\n",
    "vector_B =df_l5[(df_l5[\"cluster\"]==\"10HS\") &(df_l5[\"env\"]=='all ASVs')]['edge weight nmi (out)'].dropna().values\n",
    "\n",
    "print(\" NMI : Arctic (A) < All (B)\")\n",
    "print(\"A Arctic\",np.mean(vector_A))\n",
    "print(\"B All\",np.mean(vector_B))\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(vector_A, vector_B, equal_var=False, alternative='less')\n",
    "    \n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than alpha (indicating significance)\n",
    "if p_value < alpha:\n",
    "    print(\"p_value:\",round(p_value,3))\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"The mean of vector A is significantly greater than the mean of vector B.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"p_value:\",round(p_value,3))\n",
    "    print(\"There is not enough evidence to conclude that the mean of vector A is significantly greater than the mean of vector B.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "#results: Clearly, ASVs stable in their natural conditions (and in Atlantic conditions) display a significantly () higher centrality than average, while those predicted to be stable under Arctic conditions show a reduced centrality. \n",
    "\n",
    "vector_B = df_l5[(df_l5[\"cluster\"] == \"10HS\") & (df_l5[\"env\"] == 'all ASVs')]['edge weight nmi (out)'].dropna().values\n",
    "vector_A= df_l5[(df_l5[\"cluster\"] == \"10HS\") & (df_l5[\"env\"] == \"atlantic\")]['edge weight nmi (out)'].dropna().values\n",
    "print(\"NMI: Atlantic vs AlL\")\n",
    "print(\"All\",np.mean(vector_B))\n",
    "print(\"Atlantic\",np.mean(vector_A))\n",
    "t_statistic, p_value = stats.ttest_ind(vector_A, vector_B, equal_var=False, alternative='greater')\n",
    "\n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than alpha (indicating significance)\n",
    "if p_value < alpha:\n",
    "    print(\"p_value:\", round(p_value, 10))\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"The mean of vector A is significantly greater than the mean of vector B.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"p_value:\", round(p_value, 10))\n",
    "    print(\n",
    "        \"There is not enough evidence to conclude that the mean of vector A is significantly greater than the mean of vector B.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "#results: Clearly, ASVs stable in their natural conditions (and in Atlantic conditions) display a significantly () higher centrality than average, while those predicted to be stable under Arctic conditions show a reduced centrality. \n",
    "\n",
    "\n",
    "vector_A =df_l5[(df_l5[\"cluster\"]==\"10HS\") &(df_l5[\"env\"]==\"arctic\")]['Closeness Centrality'].dropna().values\n",
    "vector_B =df_l5[(df_l5[\"cluster\"]==\"10HS\") &(df_l5[\"env\"]=='all ASVs')]['Closeness Centrality'].dropna().values\n",
    "\n",
    "print(\" Closeness CentralityArctic (A) > All (B)\")\n",
    "print(\"A arctic\",np.mean(vector_A))\n",
    "print(\"B all \",np.mean(vector_B))\n",
    "t_statistic, p_value = stats.ttest_ind(vector_A, vector_B, equal_var=False, alternative='two-sided')\n",
    "    \n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than alpha (indicating significance)\n",
    "if p_value < alpha:\n",
    "    print(\"p_value:\",round(p_value,3))\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"The mean of vector A is significantly greater than the mean of vector B.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"p_value:\",round(p_value,3))\n",
    "    print(\"There is not enough evidence to conclude that the mean of vector A is significantly greater than the mean of vector B.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "#results: Clearly, ASVs stable in their natural conditions (and in Atlantic conditions) display a significantly () higher centrality than average, while those predicted to be stable under Arctic conditions show a reduced centrality. \n",
    "\n",
    "\n",
    "vector_A= df_l5[(df_l5[\"cluster\"] == \"10HS\") & (df_l5[\"env\"] == \"atlantic\")]['Closeness Centrality'].dropna().values\n",
    "vector_B = df_l5[(df_l5[\"cluster\"] == \"10HS\") & (df_l5[\"env\"] == 'all ASVs')]['Closeness Centrality'].dropna().values\n",
    "print(\" Closeness Centrality Atlantic vs all\")\n",
    "print(\"A atlantic\",np.mean(vector_A))\n",
    "print(\"B all\",np.mean(vector_B))\n",
    "t_statistic, p_value = stats.ttest_ind(vector_A, vector_B, equal_var=False, alternative='greater')\n",
    "\n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than alpha (indicating significance)\n",
    "if p_value < alpha:\n",
    "    print(\"p_value:\", round(p_value, 10))\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"The mean of vector A is significantly greater than the mean of vector B.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"p_value:\", round(p_value, 10))\n",
    "    print(\n",
    "        \"There is not enough evidence to conclude that the mean of vector A is significantly greater than the mean of vector B.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Venn plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_taxa = pd.read_csv(\"data/F4_euk_taxa_info_table.csv\", sep=\";\")\n",
    "species_dict = df_taxa[[\"Unnamed: 0\", \"Species\"]].set_index(\"Unnamed: 0\").to_dict()[\"Species\"]\n",
    "species_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "atlantic_color = '#B82092'\n",
    "arctic_color = '#2E7FD0'\n",
    "origin_color = '#5D5FBE'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "def plot_venn3(list1, list2, list3, species_dict):\n",
    "    plt.figure(figsize=(24, 24))\n",
    "    venn = venn3([set(list1), set(list2), set(list3)], ('Arctic', 'Atlantic = Original', 'Atlantic = Original'))\n",
    "    venn.get_patch_by_id('100').set_color(arctic_color)\n",
    "    #venn.get_patch_by_id('010').set_color(atlantic_color)\n",
    "    #venn.get_patch_by_id('001').set_color(origin_color)\n",
    "    #venn.get_patch_by_id('101').set_color('#7AAEF3')\n",
    "    venn.get_patch_by_id('011').set_color('#C58EE9')\n",
    "    #venn.get_patch_by_id('110').set_color('#D370E2')\n",
    "    venn.get_patch_by_id('111').set_color('#D9B9F9')\n",
    "    venn.get_label_by_id('100').set_text('\\n'.join([species_dict[species] for species in set(list1) - (set(list2) | set(list3))]))\n",
    "    #venn.get_label_by_id('010').set_text('\\n'.join([species_dict[species] for species in set(list2) - (set(list1) | set(list3))]))\n",
    "    #venn.get_label_by_id('001').set_text('\\n'.join([species_dict[species] for species in set(list3) - (set(list1) | set(list2))]))\n",
    "    #venn.get_label_by_id('110').set_text('\\n'.join([species_dict[species] for species in set(list1) & set(list2) - set(list3)]))\n",
    "    #venn.get_label_by_id('101').set_text('\\n'.join([species_dict[species] for species in set(list1) & set(list3) - set(list2)]))\n",
    "    venn.get_label_by_id('011').set_text('\\n'.join([species_dict[species] for species in set(list2) & set(list3) - set(list1)]))\n",
    "    venn.get_label_by_id('111').set_text('\\n'.join([species_dict[species] for species in set(list1) & set(list2) & set(list3)]))\n",
    "    #plt.title(\"Venn Diagram of Species\")\n",
    "    plt.savefig(\"figures/Sup_Figure_S14_A_ONLYSTABLE_03LW_venn_diagram_proj_vs_non_proj_stable_asvs_species.png\", dpi=300)  # Save as PNG with higher resolution\n",
    "    plt.show()\n",
    "\n",
    "# Example lists of species\n",
    "list1 = list_of_arctic_3\n",
    "list2 = list_of_atlantic_3\n",
    "list3 = list_of_atlantic_3_NON\n",
    "\n",
    "plot_venn3(list1, list2, list3, species_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l3_new_list = list_of_arctic_3 + list_of_atlantic_3 +list_of_atlantic_3_NON\n",
    "print(len(set(l3_new_list)))\n",
    "print(\"list_of_arctic_3\",len(set(list_of_arctic_3)))\n",
    "print(\"list_of_atlantic_3\",len(set(list_of_atlantic_3)))\n",
    "print(\"list_of_atlantic_3_NON\",len(set(list_of_atlantic_3_NON)))\n",
    "print(\"overlap\", len(set(list_of_arctic_3).intersection(set(list_of_atlantic_3)).intersection(set(list_of_atlantic_3_NON))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "def plot_venn3(list1, list2, list3, species_dict):\n",
    "    plt.figure(figsize=(24, 24))\n",
    "    venn = venn3([set(list1), set(list2), set(list3)], ('Arctic', 'Atlantic = Original', 'Atlantic = Original'))\n",
    "    #venn.set_colors(['B82092', '#2E7FD0', '#5D5FBE'])\n",
    "    venn.get_patch_by_id('100').set_color(arctic_color)\n",
    "    venn.get_patch_by_id('010')  #.set_color('#2E7FD0')\n",
    "    venn.get_patch_by_id('001')  #.set_color('#5D5FBE')\n",
    "    venn.get_patch_by_id('111').set_color('#D9B9F9')\n",
    "    venn.get_patch_by_id('011').set_color('#C58EE9')\n",
    "    venn.get_label_by_id('100').set_text(\n",
    "        '\\n'.join([species_dict[species] for species in set(list1) - (set(list2) | set(list3))]))\n",
    "    venn.get_label_by_id(\n",
    "        '010')  #.set_text('\\n'.join([species_dict[species] for species in set(list2) - (set(list1) | set(list3))]))\n",
    "    venn.get_label_by_id(\n",
    "        '001')  #.set_text('\\n'.join([species_dict[species] for species in set(list3) - (set(list1) | set(list2))]))\n",
    "    venn.get_label_by_id(\n",
    "        '110')  #.set_text('\\n'.join([species_dict[species] for species in set(list1) & set(list2) - set(list3)]))\n",
    "    venn.get_label_by_id(\n",
    "        '101')  #.set_text('\\n'.join([species_dict[species] for species in set(list1) & set(list3) - set(list2)]))\n",
    "    venn.get_label_by_id('011').set_text(\n",
    "        '\\n'.join([species_dict[species] for species in set(list2) & set(list3) - set(list1)]))\n",
    "    venn.get_label_by_id('111').set_text(\n",
    "        '\\n'.join([species_dict[species] for species in set(list1) & set(list2) & set(list3)]))\n",
    "    #plt.title(\"Venn Diagram of Species\")\n",
    "    plt.savefig(\"figures/Sup_Figure_S14_B_ONLYSTABLE10HS_venn_diagram_proj_vs_non_proj_stable_asvs_species.png\",\n",
    "                dpi=300)  # Save as PNG with higher resolution\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example lists of species\n",
    "list1 = list_of_arctic_10\n",
    "list2 = list_of_atlantic_10\n",
    "list3 = list_of_atlantic_10_NON\n",
    "\n",
    "plot_venn3(list1, list2, list3, species_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "l10_new_list = list_of_arctic_10 + list_of_atlantic_10 +list_of_atlantic_10_NON\n",
    "print(len(set(l10_new_list)))\n",
    "print(\"list_of_arctic_10\",len(set(list_of_arctic_10)))\n",
    "print(\"list_of_atlantic_10\",len(set(list_of_atlantic_10)))\n",
    "print(\"list_of_atlantic_10_NON\",len(set(list_of_atlantic_10_NON)))\n",
    "print(\"overlap\", len(set(list_of_arctic_10).intersection(set(list_of_atlantic_10)).intersection(set(list_of_atlantic_10_NON))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
